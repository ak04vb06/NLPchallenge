{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project | Natural Language Processing Challenge\n",
    "### Fake news classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load train and test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_raw = pd.read_csv(\n",
    "    \"training_data_lowercase.csv\",\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"label\", \"text\"]\n",
    ")\n",
    "test_data_raw = pd.read_csv(\n",
    "    \"testing_data_lowercase_nolabels.csv\",\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"label\", \"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quick EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>donald trump sends out embarrassing new year‚s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>drunk bragging trump staffer started russian c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>sheriff david clarke becomes an internet joke ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>trump is so obsessed he even has obama‚s name ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>pope francis just called out donald trump duri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  donald trump sends out embarrassing new year‚s...\n",
       "1      0  drunk bragging trump staffer started russian c...\n",
       "2      0  sheriff david clarke becomes an internet joke ...\n",
       "3      0  trump is so obsessed he even has obama‚s name ...\n",
       "4      0  pope francis just called out donald trump duri..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape (rows, columns): (34152, 2)\n",
      "Test data shape (rows, columns): (9984, 2)\n",
      "\n",
      "Fake news / real news balance:\n",
      "label\n",
      "0    0.514523\n",
      "1    0.485477\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Missing values per column:\n",
      "label    0\n",
      "text     0\n",
      "dtype: int64\n",
      "\n",
      "Number of empty text entries: 0\n"
     ]
    }
   ],
   "source": [
    "# preview\n",
    "display(train_data_raw.head())\n",
    "\n",
    "# shapes\n",
    "print(\n",
    "    f\"Training data shape (rows, columns): {train_data_raw.shape}\\n\"\n",
    "    f\"Test data shape (rows, columns): {test_data_raw.shape}\"\n",
    ")\n",
    "\n",
    "\n",
    "# fake news / real news balance\n",
    "print(\"\\nFake news / real news balance:\")\n",
    "print(train_data_raw[\"label\"].value_counts(normalize=True))\n",
    "\n",
    "# missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(train_data_raw.isnull().sum())\n",
    "\n",
    "# empty text check\n",
    "empty_texts = (train_data_raw[\"text\"].str.strip() == \"\").sum()\n",
    "print(f\"\\nNumber of empty text entries: {empty_texts}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training - validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data_raw[\"text\"]\n",
    "y = train_data_raw[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing only special characters and empty spaces\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "X_train_clean = X_train.apply(clean_text)\n",
    "X_test_clean  = X_test.apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skipping lemmatization on this model. Potential thing to try on next ones.\n",
    "We chose not to apply lemmatization because:\n",
    "- Anticipated a low impact based on the nature of the dataset. Not worth the cost\n",
    "- We could lose nuance in text relevant to fake news style\n",
    "- Lower impact of lemmatization on TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "\n",
    "\n",
    "# check column names\n",
    "print(data.head())\n",
    "\n",
    "# split between text and label\n",
    "X = data[\"text\"] \n",
    "y = data[\"label\"]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,       \n",
    "    random_state=42,\n",
    "    # to include similar proportions of spam and mail\n",
    "    stratify=y           \n",
    ")\n",
    "\n",
    "print(\"Train size:\", X_train.shape)\n",
    "print(\"Test size:\", X_test.shape)\n",
    "\n",
    "print(\"\\ndistribution in train:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "print(\"\\ndistribution in test:\")\n",
    "print(y_test.value_counts(normalize=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "def clean_text_1(text):\n",
    "    # JS\n",
    "    text = re.sub(r\"<script.*?>.*?</script>\", \" \", text, flags=re.DOTALL | re.IGNORECASE)\n",
    "    # CSS\n",
    "    text = re.sub(r\"<style.*?>.*?</style>\", \" \", text, flags=re.DOTALL | re.IGNORECASE)\n",
    "    # html comments\n",
    "    text = re.sub(r\"<!--.*?-->\", \" \", text, flags=re.DOTALL)\n",
    "    # rest of tags\n",
    "    text = re.sub(r\"<[^>]+>\", \" \", text)\n",
    "    \n",
    "    return text\n",
    "   \n",
    "X_train_clean = X_train.apply(clean_text_1)\n",
    "X_test_clean = X_test.apply(clean_text_1)\n",
    "\n",
    "# print(\"ORIGINAL:\\n\", X_train.iloc[1])\n",
    "# print(\"\\nCLEANED:\\n\", X_train_clean.iloc[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_2(text):\n",
    "    # Remove all special characters, numbers and punctuation (keep letters only)\n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n",
    "\n",
    "    # Remove all single characters (surrounded by spaces)\n",
    "    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", \" \", text)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    text = re.sub(r\"^\\s*[a-zA-Z]\\s+\", \" \", text)\n",
    "\n",
    "    # Substitute multiple spaces with single space\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    # Remove prefixed 'b'\n",
    "    text = re.sub(r\"^b\\s+\", \"\", text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    return text.lower().strip()\n",
    "\n",
    "\n",
    "X_train_clean = X_train_clean.apply(clean_text_2)\n",
    "X_test_clean = X_test_clean.apply(clean_text_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    email_words = text.split()\n",
    "    email_words = [word for word in email_words if word not in stop_words]\n",
    "    return \" \".join(email_words)\n",
    "\n",
    "X_train_clean = X_train_clean.apply(remove_stopwords)\n",
    "X_test_clean = X_test_clean.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "def lemmatize_text(text):\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "X_train_clean = X_train_clean.apply(lemmatize_text)\n",
    "X_test_clean  = X_test_clean.apply(lemmatize_text)\n",
    "\n",
    "i = 1\n",
    "print(\"FINAL CLEANED:\\n\", X_train_clean.iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "X_train_ham  = X_train_clean[y_train == 0]\n",
    "X_train_spam = X_train_clean[y_train == 1]\n",
    "\n",
    "# top_10_ham\n",
    "vectorizer_ham = CountVectorizer()\n",
    "X_ham_bow = vectorizer_ham.fit_transform(X_train_ham)\n",
    "\n",
    "ham_word_counts = X_ham_bow.sum(axis=0)\n",
    "ham_words_freq = zip(\n",
    "    vectorizer_ham.get_feature_names_out(),\n",
    "    ham_word_counts.A1\n",
    ")\n",
    "\n",
    "ham_words_freq = sorted(ham_words_freq, key=lambda x: x[1], reverse=True)\n",
    "top_10_ham = ham_words_freq[:10]\n",
    "\n",
    "print(top_10_ham)\n",
    "\n",
    "# top_10_spam\n",
    "vectorizer_spam = CountVectorizer()\n",
    "X_spam_bow = vectorizer_spam.fit_transform(X_train_spam)\n",
    "\n",
    "spam_word_counts = X_spam_bow.sum(axis=0)\n",
    "spam_words_freq = zip(\n",
    "    vectorizer_spam.get_feature_names_out(),\n",
    "    spam_word_counts.A1\n",
    ")\n",
    "\n",
    "spam_words_freq = sorted(spam_words_freq, key=lambda x: x[1], reverse=True)\n",
    "top_10_spam = spam_words_freq[:10]\n",
    "\n",
    "print(top_10_spam)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",r\"\\$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()\"\"\"\n",
    "\n",
    "money_symbol_list = \"|\".join([\"euro\", \"dollar\", \"pound\", \"€\", r\"\\$\"])\n",
    "\n",
    "suspicious_words = \"|\".join([\n",
    "    \"free\", \"cheap\", \"sex\", \"money\", \"account\", \"bank\",\n",
    "    \"fund\", \"transfer\", \"transaction\", \"win\", \"deposit\", \"password\"\n",
    "])\n",
    "\n",
    "X_train_money_mark = X_train_clean.str.contains(money_symbol_list, regex=True).astype(int)\n",
    "X_train_suspicious_words = X_train_clean.str.contains(suspicious_words, regex=True).astype(int)\n",
    "X_train_text_len = X_train_clean.apply(len)\n",
    "\n",
    "X_test_money_mark = X_test_clean.str.contains(money_symbol_list, regex=True).astype(int)\n",
    "X_test_suspicious_words = X_test_clean.str.contains(suspicious_words, regex=True).astype(int)\n",
    "X_test_text_len = X_test_clean.apply(len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# train\n",
    "X_train_bow = vectorizer.fit_transform(X_train_clean)\n",
    "# same as \n",
    "# vectorizer.fit(X_train_clean)\n",
    "# X_train_bow = vectorizer.transform(X_train_clean)\n",
    "\n",
    "# test\n",
    "X_test_bow = vectorizer.transform(X_test_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "# load vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "#vectorize dataset\n",
    "#train\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_clean)\n",
    "#test\n",
    "X_test_tfidf  = tfidf_vectorizer.transform(X_test_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "#init classifier\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "\n",
    "#train model \n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# predict for test\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to **find the most relevant features**.\n",
    "\n",
    "For example, you can test the following options and check which of them performs better:\n",
    "- Using \"Bag of Words\" only\n",
    "- Using \"TF-IDF\" only\n",
    "- Bag of Words + extra flags (money_mark, suspicious_words, text_len)\n",
    "- TF-IDF + extra flags\n",
    "\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "ironhack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
